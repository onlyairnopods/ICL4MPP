{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/baoyan2015/article/details/135777614"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "# model_name_or_id = \"AI4Chem/ChemLLM-7B-Chat\" \n",
    "# model_name_or_id = \"AI4Chem/ChemLLM-20B-Chat-DPO\"\n",
    "# model_name_or_id = \"AI4Chem/ChemLLM-20B-Chat-SFT\"\n",
    "model_name_or_id = \"AI4Chem/ChemLLM-7B-Chat-1_5-SFT\"\n",
    "\n",
    "# model_name_or_id = \"X-LANCE/ChemDFM-13B-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009779930114746094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 8,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b3aecf87a74086b193f5488116c74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_id, torch_dtype=torch.float16, trust_remote_code=True,\n",
    "                                             device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_id,trust_remote_code=True)\n",
    "\n",
    "# model = LlamaForCausalLM.from_pretrained(model_name_or_id, torch_dtype=torch.float16, trust_remote_code=True,\n",
    "#                                             device_map=\"auto\")\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_name_or_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当使用大型语言模型（如 GPT-2、GPT-3、BERT 等）的 generate 方法直接产生文本时，通常返回的是文本的 token ID 序列。  \n",
    "为了获得每个 token 的生成概率，你需要使用模型的 logits 输出。logits 是模型在softmax层之前的输出，表示模型对每个可能的下一个 token 的置信度。通过对这些 logits 应用softmax函数，可以得到概率分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 09:19:59.133534: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 09:19:59.993451: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: 26444, Probability: 0.46754440665245056\n",
      "Token ID: 1053, Probability: 0.9870383739471436\n",
      "Token ID: 410, Probability: 0.9968327879905701\n",
      "Token ID: 15810, Probability: 0.9913532137870789\n",
      "Token ID: 5718, Probability: 0.9688257575035095\n",
      "Token ID: 281, Probability: 0.5558463931083679\n",
      "Token ID: 707, Probability: 0.19871832430362701\n",
      "Token ID: 4131, Probability: 0.45601770281791687\n",
      "Token ID: 14018, Probability: 0.959482729434967\n",
      "Token ID: 38648, Probability: 0.8549602031707764\n",
      "Token ID: 26444, Probability: 0.8256170749664307\n",
      "Token ID: 1053, Probability: 0.9749144911766052\n",
      "Token ID: 410, Probability: 0.9972091317176819\n",
      "Token ID: 15810, Probability: 0.9762767553329468\n",
      "Token ID: 5718, Probability: 0.9929332733154297\n"
     ]
    }
   ],
   "source": [
    "# encoder input text\n",
    "input_text = \"The quick brown fox\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# generate output text and get logits\n",
    "outputs = model.generate(input_ids,\n",
    "                         max_length=20,\n",
    "                         output_scores=True, # return every generated token's logits\n",
    "                         return_dict_in_generate=True)\n",
    "\n",
    "# extract logits\n",
    "logits = outputs.scores\n",
    "\n",
    "# compute probability\n",
    "probs = [torch.softmax(log, dim=-1) for log in logits]\n",
    "\n",
    "# get token ID of the generated output and corresponding probabilities\n",
    "generated_ids = outputs.sequences\n",
    "\n",
    "for i, token_id in enumerate(generated_ids[0][ len(input_ids[0]): ]):\n",
    "    token_prob = probs[i][0, token_id].item()\n",
    "    print(f\"Token ID: {token_id}, Probability: {token_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 92544]), tensor([26444]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = torch.softmax(logits[0], dim=-1)\n",
    "idx = prob.argmax(-1) # 使用argmax函数找到概率分布中最大值的索引。-1参数表示在最后一个维度上进行操作，即在每个样本的概率分布中找到最大值的索引。这通常用于确定模型预测的类别。\n",
    "prob.shape, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(26444)\n",
      "tensor(0.4675)\n",
      "▁jumped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(0.5325)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result = [prob[i][y] if 'yes'in tokenizer.convert_ids_to_tokens(int(y.detach())).lower() else 1-prob[i][y] for i, y in enumerate(idx)]\n",
    "for i, y in enumerate(idx):\n",
    "    print(i, y)\n",
    "    print(prob[i][y])\n",
    "    result = []\n",
    "    print(tokenizer.convert_ids_to_tokens(int(y.detach())).lower())\n",
    "    if 'yes'in tokenizer.convert_ids_to_tokens(int(y.detach())).lower():\n",
    "        result.append(prob[i][y])\n",
    "    else:\n",
    "        result.append(1-prob[i][y])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09:27:04] Explicit valence for atom # 0 N, 5, is greater than permitted\n",
      "Failed to featurize datapoint 7, None. Appending empty array\n",
      "Exception message: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\n",
      "did not match C++ signature:\n",
      "    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True, bool includeAtomMaps=True)\n",
      "[09:27:04] Can't kekulize mol.  Unkekulized atoms: 9\n",
      "Failed to featurize datapoint 302, None. Appending empty array\n",
      "Exception message: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\n",
      "did not match C++ signature:\n",
      "    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True, bool includeAtomMaps=True)\n",
      "[09:27:05] Can't kekulize mol.  Unkekulized atoms: 4\n",
      "Failed to featurize datapoint 1219, None. Appending empty array\n",
      "Exception message: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\n",
      "did not match C++ signature:\n",
      "    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True, bool includeAtomMaps=True)\n",
      "[09:27:05] Can't kekulize mol.  Unkekulized atoms: 4\n",
      "Failed to featurize datapoint 1220, None. Appending empty array\n",
      "Exception message: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\n",
      "did not match C++ signature:\n",
      "    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True, bool includeAtomMaps=True)\n",
      "Exception message: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1484,) + inhomogeneous part.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: C[NH+]1C[C@H](C(=O)N[C@]2(C)O[C@@]3(O)[C@@H]4CCCN4C(=O)[C@H](Cc4ccccc4)N3C2=O)C[C@@H]2c3cccc4[nH]cc(c34)C[C@H]21 y: [1. 0.] id: C[C@@]1(C(=O)N2[C@H](C(=O)N3CCC[C@H]3[C@@]2(O1)O)Cc4ccccc4)NC(=O)[C@@H]5C[C@@H]6c7cccc8c7c(c[nH]8)C[C@H]6[NH+](C5)C\n",
      "X: C[C@]1(Cn2ccnn2)[C@H](C(=O)[O-])N2C(=O)C[C@H]2S1(=O)=O y: [1. 0.] id: C[C@@]1([C@@H](N2[C@H](S1(=O)=O)CC2=O)C(=O)[O-])Cn3ccnn3\n",
      "X: C[NH+]1CCC[C@@H]1CCO[C@](C)(c1ccccc1)c1ccc(Cl)cc1 y: [1. 0.] id: C[C@@](c1ccccc1)(c2ccc(cc2)Cl)OCC[C@H]3CCC[NH+]3C\n",
      "X: Nc1nc(NC2CC2)c2ncn([C@H]3C=C[C@@H](CO)C3)c2n1 y: [1. 0.] id: c1nc2c(nc(nc2n1[C@@H]3C[C@@H](C=C3)CO)N)NC4CC4\n",
      "X: OC[C@H]1O[C@@H](n2cnc3c2NC=[NH+]C[C@H]3O)C[C@@H]1O y: [1. 0.] id: c1nc2c(n1[C@H]3C[C@@H]([C@H](O3)CO)O)NC=[NH+]C[C@H]2O\n"
     ]
    }
   ],
   "source": [
    "import deepchem\n",
    "import deepchem.molnet\n",
    "tasks, datasets, transformers = deepchem.molnet.load_clintox(featurizer=deepchem.feat.RawFeaturizer(True),\n",
    "                                                             splitter='scaffold', reload=True,\n",
    "                                                             data_dir='./data/clintox_data',\n",
    "                                                             save_dir='./data/clintox_datasets')\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "for i in range(5):\n",
    "    print(f\"X: {test_dataset.X[i]} y: {test_dataset.y[i]} id: {test_dataset.ids[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = list(test_dataset.ids)\n",
    "label = list(np.array(i[1]) for i in test_dataset.y)\n",
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0, token_id: 26444\n",
      "i: 1, token_id: 1053\n",
      "i: 2, token_id: 410\n",
      "i: 3, token_id: 15810\n",
      "i: 4, token_id: 5718\n",
      "i: 5, token_id: 281\n",
      "i: 6, token_id: 707\n",
      "i: 7, token_id: 4131\n",
      "i: 8, token_id: 14018\n",
      "i: 9, token_id: 38648\n",
      "i: 10, token_id: 26444\n",
      "i: 11, token_id: 1053\n",
      "i: 12, token_id: 410\n",
      "i: 13, token_id: 15810\n",
      "i: 14, token_id: 5718\n"
     ]
    }
   ],
   "source": [
    "for i, token_id in enumerate(generated_ids[0][ len(input_ids[0]): ]):\n",
    "    print(f\"i: {i}, token_id: {token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   918,  4131, 14018, 38648, 26444,  1053,   410, 15810,  5718,\n",
       "           281,   707,  4131, 14018, 38648, 26444,  1053,   410, 15810,  5718]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   918,  4131, 14018, 38648]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([26444,  1053,   410, 15810,  5718,   281,   707,  4131, 14018, 38648,\n",
       "        26444,  1053,   410, 15810,  5718])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids[0][ len(input_ids[0]): ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([[0.0000e+00, 0.0000e+00, 2.7255e-05,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00]]),\n",
       "  tensor([[0.0000e+00, 0.0000e+00, 1.7375e-06,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00]]),\n",
       "  tensor([[0.0000e+00, 0.0000e+00, 1.4404e-07,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00]]),\n",
       "  tensor([[0.0000e+00, 0.0000e+00, 1.1156e-07,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00]]),\n",
       "  tensor([[0.0000e+00, 0.0000e+00, 1.7976e-07,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00]]),\n",
       "  tensor([[0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000]]),\n",
       "  tensor([[1.3865e-31, 1.1124e-28, 5.5816e-04,  ..., 7.6454e-29, 6.9717e-32,\n",
       "           0.0000e+00]]),\n",
       "  tensor([[1.0920e-38, 8.5892e-38, 6.7213e-06,  ..., 8.8486e-35, 1.6047e-37,\n",
       "           0.0000e+00]]),\n",
       "  tensor([[0.0000e+00, 0.0000e+00, 7.5697e-06,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00]]),\n",
       "  tensor([[0.0000e+00, 0.0000e+00, 1.1121e-05,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00]]),\n",
       "  tensor([[0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000]]),\n",
       "  tensor([[0.0000e+00, 0.0000e+00, 1.2681e-05,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00]]),\n",
       "  tensor([[0.0000e+00, 0.0000e+00, 1.7554e-06,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00]]),\n",
       "  tensor([[0.0000e+00, 0.0000e+00, 2.2067e-06,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00]]),\n",
       "  tensor([[0.0000e+00, 0.0000e+00, 2.8818e-06,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00]])],\n",
       " 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs, len(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000e+00, 0.0000e+00, 2.7255e-05,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00]]),\n",
       " torch.Size([1, 92544]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0], probs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.674778229078584e-08"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0][0, 7560].item() # 7560为ChemLLM词表中Yes对应的id, 查看输出yes的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4675)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0][0, 26444]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从 Token ID 获取对应的文本  \n",
    "\n",
    "若要从已知的 token ID 获取对应的文本，你需要使用加载的 tokenizer 对象的 decode 方法。以下是如何操作的步骤：\n",
    "1. 加载预训练的 tokenizer。\n",
    "2. 使用 decode 方法将 token ID 转换为文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " jumped over the lazy dog. The quick brown fox jumped over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "token_ids = generated_ids[0][ len(input_ids[0]): ]\n",
    "\n",
    "# use `decode` method to convert token ID to text\n",
    "text = tokenizer.decode(token_ids)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 16:43:00.721890: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-21 16:43:01.661077: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes No\n"
     ]
    }
   ],
   "source": [
    "token_ids = [7560, 2458]\n",
    "\n",
    "# use `decode` method to convert token ID to text\n",
    "text = tokenizer.decode(token_ids)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9583])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoder input text\n",
    "input_text = \"Yes\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt', add_special_tokens=False)\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 15:03:58.915601: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-29 15:03:59.761033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Yes NoYesNo</s>\n"
     ]
    }
   ],
   "source": [
    "token_ids = [7560, 2458, 9583, 2917, 2]\n",
    "\n",
    "# use `decode` method to convert token ID to text\n",
    "text = tokenizer.decode(token_ids)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 09:43:03.340089: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-23 09:43:04.160541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes yes No noYesNo\n"
     ]
    }
   ],
   "source": [
    "# ChemDFM\n",
    "token_ids = [3869, 4874, 1939, 694, 8241, 3782]\n",
    "\n",
    "# use `decode` method to convert token ID to text\n",
    "text = tokenizer.decode(token_ids)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LZZ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
